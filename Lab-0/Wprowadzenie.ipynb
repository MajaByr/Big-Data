{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fc88afc-fe76-45cc-ac40-52ee63cb935c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Apache Spark wprowadzenie\n",
    "<br/>\n",
    "* Co to jest Databricks?\n",
    "* Czym jest Apache Spark?\n",
    "* Co to jest RDD\n",
    "* Transformacje i Akcje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adc1572f-339b-411f-82ca-3c15aff66b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Co to jest Databricks?\n",
    "<br/>\n",
    "* Jest to łatwa w konfiguracji platforma analityczna, zapewniająca interaktywną przestrzeń roboczą ze wsparciem dla wielu platform danych i aplikacji.\n",
    "* W całości oparta na projekcie 'Open Source' Apache Spark.\n",
    "* Zoptymalizowana pod kątem platformy usług w chmurze Microsoft Azure oraz AWS.\n",
    "* Azure Databricks łączy się z wieloma usługami w Azure:\n",
    "  - Power BI\n",
    "  - Azure IoT Hub\n",
    "  - Azure Event Hubs\n",
    "  - Klastrami Azure HDInsight Kafka\n",
    "  - Azure Blob Storage\n",
    "  - Azure Data Lake Store\n",
    "  - Azure Synapse\n",
    "  - Azure Cosmos DB.\n",
    "* Jeśli chcesz wiedzieć więcej szczegółów zapraszam na stronę <a href=\"https://docs.microsoft.com/pl-pl/azure/databricks/\">  Databricks</a> \n",
    "* Platforma analityczna Databricks składa się z następujących elementów:\n",
    "* **Databricks Środowisko wykonawcze** obejmuje Apache Spark, ale dodaje również szereg składników i aktualizacji, które znacznie poprawiają użyteczność, wydajność i bezpieczeństwo analizy dużych zbiorów danych:\n",
    "\n",
    "    - **Databricks Delta**, to ujednolicony system zarządzania danymi, który zapewnia niezawodność i wydajność. Zapewnia transakcje ACID, zoptymalizowane układy i indeksy oraz ulepszenia silnika wykonawczego do tworzenia potoków danych.\n",
    "    Możesz czytać i zapisywać dane przechowywane w Databricks Delta przy użyciu tych samych znanych interfejsów API wsadowych i strumieniowych SQL Apache Spark, których używasz do pracy z tabelami Hive lub katalogami DBFS\n",
    "    - Zainstalowane **biblioteki** Java, Scala, Python i R.\n",
    "    - Ubuntu i towarzyszące mu biblioteki systemowe\n",
    "    - **Biblioteki GPU** dla klastrów obsługujących procesory graficzne\n",
    "    - Usługi Databricks, które integrują się z innymi komponentami platformy, takimi jak notebooki, zadania i menedżer klastra\n",
    "  \n",
    "* **Przestrzeń robocza**\n",
    "  - Obszar roboczy Databricks to środowisko umożliwiające dostęp do wszystkich zasobów Databricks. Przestrzeń robocza organizuje obiekty (notatniki, biblioteki i eksperymenty) w folderach i zapewnia dostęp do danych oraz zasobów obliczeniowych, takich jak klastry i zadania.\n",
    "  \n",
    "\n",
    "  * Databricks Klastry - Najważniejszy element, czyli zasoby służące do uruchamiania aplikacji. klastry zapewniają moc obliczeniową. Możesz wybrać ile pamięci, CPU i ile wykonawców potrzebujesz. \n",
    "  * Databricks Notebooki - Interaktywne notebooki obsługiwane z poziomu przeglądarki. Możliwość uruchamiania kodu, wizualizacji oraz tworzenia notatek.\n",
    "  * Databricks Jobs - możliwość stworzenie harmonogramu, który automatycznie uruchamia aplikację np jar. \n",
    "  * Databricks <a href:\"https://kb.databricks.com/?_ga=2.170917152.1296934561.1599721015-80976339.1584632418\"> Baza Wiedzy</a>  mnóstwo ważnych informacji i przykładów jak używać Databricks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29bf8e78-a234-4bc3-a6c4-7f627e6f2794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Czym jest Apache Spark?\n",
    "\n",
    "Spark jest silnikiem do równoległego i rozproszonego przetwarzania dużych ilości danych. Jest to narzędzie typu 'Open Srouce'. Składa się on z wielu komponentów. \n",
    "* Elementy Sparka to:\n",
    "  - Spark SQL\n",
    "  - DataFrame\n",
    "  - Przesyłanie strumieniowe\n",
    "  - Bibliotekę MLib\n",
    "  - Wykresy i obliczenia GraphFrames, GraphX\n",
    "  - Interfejs API Spark Core \n",
    "  - Obsługuje języki R, SQL, Python, Scala i Java.\n",
    "  \n",
    "*  Interfejsy API pozwalające na wykonywanie transformacji na danych. Sa one dostępne w kilku językach.\n",
    "* Ramki danych (DataFrames) tworzą główną warstwę abstrakcji rozproszonych zbiorów danych. Dzięki nim praca z danymi jest dużo prostsza, wszystkie kolumny mają nazwę.\n",
    "* Spark RDD (Resilient Distributed Dataset). Jest on partycjonowany i rozproszony zbiór obiektów danych w całym klastrze.\n",
    "* Spark ML czyli biblioteki stworzone do nauczania maszynowego.\n",
    "* Analiza Wykresów wspiera GraphFrames and GraphX - Jest to pakiet dla Sparka dający możliwość równoległego przetwarzania wykresów danych. Ta rozszerzona funkcjonalność obejmuje wyszukiwanie motywów, serializację opartą na DataFrame i wysoce ekspresyjne zapytania wykresowe.\n",
    "* The Streaming APIs czyli przetwarzanie strumieniowe. Spark Streaming to rozszerzenie podstawowego interfejsu Spark API, które umożliwia skalowalne, wysokoprzepustowe i odporne na błędy przetwarzanie strumieni danych na żywo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9f81d1-1943-4a1f-8b55-b012153e4c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Zbiory danych\n",
    "\n",
    "Spark oferuje różne formy abstrakcji danych.\n",
    "\n",
    "-   ****The RDD (Resilient Distributed Dataset)****\n",
    "   - RDD czyli (Resilient Distributed Dataset) jest to podstawowy element struktury danych w Sparku. Jest to niezmienny, rozproszony zbiór obiektów.  Istnieją dwa sposoby tworzenia RDDs: \n",
    "  - Zrównoleglenie istniejącej kolekcji w sterowniku \n",
    "  - Odwołanie się do zestawu danych w zewnętrznym systemie dysków\n",
    "  - Jest to struktura odporna na awarie, dzięki pomocy RDD lineage graph.  W momencie wykrycia awarii popsute partycje mogą być odtworzone.\n",
    "  - Dane są rozproszone w wielu węzłach w całym klastrze.\n",
    "\n",
    "-   ****The DataFrame****\n",
    "    - Są podobne do dataframe’ow z R i pandas Python\n",
    "    \n",
    "-   ****The Dataset****\n",
    "    - Silnie typowany data frame\n",
    "    - Dataset jest to partycjonowana kolekcja prymitywnych wartości.\n",
    "\n",
    "    \n",
    "| Language | Main Abstraction                                |\n",
    "|----------|-------------------------------------------------|\n",
    "| Scala    | Dataset[T] & DataFrame (alias for Dataset[Row]) |\n",
    "| Java     | Dataset[T]                                      |\n",
    "| Python*  | DataFrame                                       |\n",
    "| R*       | DataFrame                                       |\n",
    "\n",
    "Ponieważ Python i R są językami dynamicznie typowanymi, dostępne są tylko RDD i DataFrame'y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0580c674-0e9e-4fc8-a4f9-1aee79c69456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Transformacje\n",
    "\n",
    "Transformacje to operacje, które nie zostaną wykonane w momencie ich napisania/uruchomienia - zostaną uruchomione dopiero gdy wywołasz **akcję**. Przykładową transformację może by zamiana typu danych int na float albo przefiltrowanie zbioru danych. Lista przykładowych transformacji w tabeli poniżej.\n",
    "\n",
    "|**Transformation**|**Meaning**|\n",
    "|--|--|\n",
    "|**map**(func)|Zwraca rozproszony zestaw danych utworzony przez przekazanie każdego elementu danych do funkcji.|\n",
    "|**filter**(func)|Zwraca nowy zestaw danych utworzony przez wybranie tych elementów, dla których funkcja zwrócila wartość true.|\n",
    "|**flatMap**(func)| Podobny do funkcji (map), z wyjątkiem tego, że każdy element wejściowy można mapować na jeden lub więcej elementów wyjściowych. Więc funkcja może zwrócić sekwencje.|;\n",
    "|**reduceByKey**(func, [numPartitions])| Jeśli uruchamiasz funkcję na zestawie danych w formacie (K:klucz, V: Wartość) i funkcja zwraca zestaw danych w formacie (K:klucz, V: Wartość). Kiedy wartość każdego klucza jest agregowana przy użyciu funkcji redukującej, która musi byc typem (V,V) => V. Można ustawić liczbę zadań redukujących przy użyciu drugiego parametru. |\n",
    "|**aggregateByKey**(zeroValue)(seqOp, combOp, [numPartitions])| Jeśli uruchamiasz funkcję na zestawie danych w formacie (K:klucz, V: Wartość) i funkcja zwraca zestaw danych w formacie (K, U), gdzie wartości są agregowane przy użyciu łączonych funkcji i neutralnej wartości \"zero\". Pozwala to na agregowanie typów, które są inne niż typy wejściowe. Tak jak w przypadku 'groupByKey', Można ustawić liczbę zadań redukujących przy użyciu drugiego parametru.|\n",
    "|**sortByKey**([ascending], [numPartitions])| Jeśli uruchamiasz funkcję na zestawie danych w formacie (K:Klucz, V: Wartość), gdzie K implementuje sortowanie i zwraca posortowany zestaw danych rosnoąco lub malejąco.|\n",
    "|**coalesce**(numPartitions)|Zmniejsza ilość partycji w RDD do zdefiniowanej liczby. Użyteczne, kiedy używasz filtra na dużym zestawie danych, pozwala uniknąc pustych partycji. dataset.|\n",
    "|**repartition**(numPartitions)| Może być użyta do zmiany ilośći partycji RDD.  Trzeba pamiętać, że funkcja ta wykonuje operacje 'shuffle' czyli prześle dane przez sieć, co jest kosztowną operacją.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de0284b-ddcd-4583-b17c-42f752931842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Akcje\n",
    "\n",
    "Akcje to komendy, których wynik jest obliczany w chwili ich uruchomienia. Takie uruchomienie obejmuje też wykonanie wszystkich transformacji, które poprzedzały akcję. Akcja składa się z jednego lub więcej zadań, które zostaną wykonane równolegle przez wiele wykonawców.\n",
    "\n",
    "|Action|Meaning|\n",
    "|--|--|\n",
    "|**reduce**(func)| Agreguje elementy zestawu danych używając fukcji, która pobiera dwa parametry i zwraca jeden. |\n",
    "|**collect**()| Zwraca wszystkie elementy zestawu danych jako tablicę. Wszystkie dane są zbierane na sterowniku. Przy dużej ilości danych może to być problemem i spowodować niestabilność sterownika.|\n",
    "|**count**()| Zwraca liczbę elementów zjadujących się w zestawie danych.|\n",
    "|**first**()| Zwraca pierszy wiersz zestawu danych.|\n",
    "|**take**(n)| Zwraca tablicę z n ilością elementów w zestawie danych. |\n",
    "|**takeSample**(withReplacement, num, [seed])| Zwraca tablicę z losową liczbą elementów z zestawu danych. Można użyc zamiennika i generatorem liczb losowych. |\n",
    "|**foreach**(func)| Uruchamia funkcję na każdym elemencie zestawu danych.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee67bb1a-8a22-4182-bd87-f8cc19181df9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Adaptacyjne wykonywanie zapytań (Adaptive Query Execution) \n",
    "\n",
    "Wykonuje Optymalizację zapytań Podczas wykonywania zapytań. \n",
    "\n",
    "Dostosowywanie planów zapytań na podstawie statystyk 'runtime' zbieranych w procesie wykonywania zapytań\n",
    "\n",
    "Dostępne w wersji Spark 3.0 i Databricks Runtime 7.0, zalety\n",
    "* Dynamicznie zmienia łączenie sortowania przez łączenie z rozgłaszaniem (sort merge join into broadcast hash join).\n",
    "* Dynamicznie łączy partycje (łączy małe partycje w partycje o rozsądnym rozmiarze) po wymianie losowej (shuffle). Bardzo małe zadania mają gorszą przepustowość IO i są bardziej narażone na obciążenie związane z harmonogramowaniem i konfiguracją zadań.\n",
    "* Dynamicznie obsługuje pochylenie w łączeniu sortowania (sort merge join), scalaniu i mieszaniu łączeń (hash join), dzieląc (i replikując w razie potrzeby) dzieli na zadania o mniej więcej równych rozmiarach.\n",
    "* Dynamicznie wykrywa i propaguje puste relacje.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1. Wprowadzenie",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
